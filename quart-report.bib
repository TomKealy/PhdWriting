@article{Candes2006,
author = {Cand\`{e}s, Emmanuel J and Romberg, Justin and Tao, Terence},
file = {:home/tk12098/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cand\`{e}s, Romberg, Tao - 2006 - Robust Uncertainty Principles Exact Signal Frequency Information.pdf:pdf},
number = {2},
pages = {489--509},
title = {{Robust Uncertainty Principles : Exact Signal Frequency Information}},
volume = {52},
year = {2006}
}
@article{Donoho2006,
abstract = {Suppose x is an unknown vector in Ropfm (a digital image or signal); we plan to measure n general linear functionals of x and then reconstruct. If x is known to be compressible by transform coding with a known transform, and we reconstruct via the nonlinear procedure defined here, the number of measurements n can be dramatically smaller than the size m. Thus, certain natural classes of images with m pixels need only n=O(m1/4log5/2(m)) nonadaptive nonpixel samples for faithful recovery, as opposed to the usual m pixel samples. More specifically, suppose x has a sparse representation in some orthonormal basis (e.g., wavelet, Fourier) or tight frame (e.g., curvelet, Gabor)-so the coefficients belong to an lscrp ball for 0<ples1. The N most important coefficients in that expansion allow reconstruction with lscr2 error O(N1/2-1p/). It is possible to design n=O(Nlog(m)) nonadaptive measurements allowing reconstruction with accuracy comparable to that attainable with direct knowledge of the N most important coefficients. Moreover, a good approximation to those N important coefficients is extracted from the n measurements by solving a linear program-Basis Pursuit in signal processing. The nonadaptive measurements have the character of "random" linear combinations of basis/frame elements. Our results use the notions of optimal recovery, of n-widths, and information-based complexity. We estimate the Gel'fand n-widths of lscrp balls in high-dimensional Euclidean space in the case 0<ples1, and give a criterion identifying near- optimal subspaces for Gel'fand n-widths. We show that "most" subspaces are near-optimal, and show that convex optimization (Basis Pursuit) is a near-optimal way to extract information derived from these near-optimal subspaces},
archivePrefix = {arXiv},
arxivId = {1204.4227v1},
author = {Donoho, D L},
doi = {10.1109/TIT.2006.871582},
eprint = {1204.4227v1},
file = {:home/tk12098/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Donoho - 2006 - Compressed sensing.pdf:pdf},
institution = {Carnegie Mellon University},
isbn = {9781467300469},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
number = {4},
pages = {1289--1306},
publisher = {IEEE},
title = {{Compressed sensing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1614066},
volume = {52},
year = {2006}
}
