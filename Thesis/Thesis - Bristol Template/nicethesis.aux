\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*{\memsetcounter}[2]{}
\bibstyle{plain}
\catcode `"\active 
\catcode `<\active 
\catcode `>\active 
\@nameuse{es@quoting}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\par \penalty \@M \unhbox \voidb@x \hbox {}\hfill {\nag@@warning@vi  \bfseries  Page}\par \penalty \@M }
\citation{Strategy2013}
\citation{Burbidge2007}
\citation{Tibshirani1996}
\citation{Tibshirani1996}
\citation{Ji2008}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{v}{section*.1}}
\citation{atia2}
\@writefile{lof}{\par \penalty \@M \textbf  {{\scshape  Figure} \hfill Page}\par \penalty \@M }
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {1}Introduction}{1}{chapter.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{1}{section.1.1}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}Classical Sensing}{5}{chapter.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{5}{section.2.1}}
\citation{Strategy2013}
\citation{Strategy2013}
\citation{Burbidge2007}
\citation{Burbidge2007}
\citation{Candes2006}
\citation{donoho2}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A digram of current Spectral allocation \cite  {Strategy2013}}}{6}{figure.2.1}}
\newlabel{spectrumalloc}{{\M@TitleReference {2.1}{A digram of current Spectral allocation \cite  {Strategy2013}}}{6}{A digram of current Spectral allocation \cite {Strategy2013}}{figure.2.1}{}}
\citation{Donoho}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A snapshot of frequency utilisation in various areas: many frequencies are not used at all, whilst there is significant activity on others \cite  {Burbidge2007}}}{7}{figure.2.2}}
\newlabel{frequtil}{{\M@TitleReference {2.2}{A snapshot of frequency utilisation in various areas: many frequencies are not used at all, whilst there is significant activity on others \cite  {Burbidge2007}}}{7}{A snapshot of frequency utilisation in various areas: many frequencies are not used at all, whilst there is significant activity on others \cite {Burbidge2007}}{figure.2.2}{}}
\citation{mishali2010theory}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Classical Sensing}{9}{section.2.2}}
\citation{yucek2009survey}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Narrowband Spectrum Sensing}{10}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Classical Spectrum Sensing}{10}{subsection.2.3.1}}
\newlabel{h1}{{\M@TitleReference {2.3.1}{Classical Spectrum Sensing}}{10}{Classical Spectrum Sensing}{equation.2.3.7}{}}
\newlabel{h2}{{\M@TitleReference {2.3.1}{Classical Spectrum Sensing}}{10}{Classical Spectrum Sensing}{equation.2.3.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{Energy Detection}{11}{section*.2}}
\citation{yucek2009survey}
\citation{yucek2009survey}
\citation{xie2009optimal}
\citation{hamdi2010impact}
\citation{sahai2004some}
\citation{zhang2011adaptive}
\citation{olivieri2005scalable}
\citation{tandra2008snr}
\citation{oude2011lowering}
\citation{ye2007spectrum}
\citation{kim2007cyclostationary}
\@writefile{toc}{\contentsline {subsubsection}{Cyclostationary Feature Detection}{13}{section*.3}}
\newlabel{cyclic-covarience}{{\M@TitleReference {2.3.1}{Cyclostationary Feature Detection}}{13}{Cyclostationary Feature Detection}{equation.2.3.18}{}}
\citation{Ghozzi2006}
\citation{lunden2007spectrum}
\citation{cabric2004implementation}
\citation{vcabric2005physical}
\citation{Ghozzi2006}
\citation{cabric2004implementation}
\citation{yucek2009survey}
\newlabel{c1}{{\M@TitleReference {2.3.1}{Cyclostationary Feature Detection}}{14}{Cyclostationary Feature Detection}{equation.2.3.22}{}}
\newlabel{c2}{{\M@TitleReference {2.3.1}{Cyclostationary Feature Detection}}{14}{Cyclostationary Feature Detection}{equation.2.3.23}{}}
\citation{bhargavi2010performance}
\@writefile{toc}{\contentsline {subsubsection}{Matched Filtering}{15}{section*.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Distributed Approaches to Spectrum Sensing}{15}{subsection.2.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{Limitations}{15}{section*.5}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {3}Compressive Sensing}{17}{chapter.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction and Preliminaries}{17}{section.3.1}}
\newlabel{sec:intro}{{\M@TitleReference {3.1}{Introduction and Preliminaries}}{17}{Introduction and Preliminaries}{section.3.1}{}}
\citation{watkincandes}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}RIP and Stable Embeddings}{18}{subsection.3.1.1}}
\citation{shalev2014understanding}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A visualisation of the Compressive Sensing problem as an under-determined system}}{19}{figure.3.1}}
\newlabel{l1l2}{{\M@TitleReference {3.1}{A visualisation of the Compressive Sensing problem as an under-determined system}}{19}{A visualisation of the Compressive Sensing problem as an under-determined system}{figure.3.1}{}}
\newlabel{def:RIP}{{\M@TitleReference {3.1.2}{RIP and Stable Embeddings}}{19}{RIP}{theorem.3.1.2}{}}
\newlabel{def:RIP}{{\M@TitleReference {3.1.2}{RIP and Stable Embeddings}}{19}{RIP}{equation.3.1.6}{}}
\newlabel{def:d-stable}{{\M@TitleReference {3.1.7}{RIP and Stable Embeddings}}{20}{\(\delta \)-stable embedding}{equation.3.1.8}{}}
\citation{Candes2006}
\citation{davenport2010signal}
\citation{baraniuk2008simple}
\citation{baraniuk2008simple}
\newlabel{minsamples}{{\M@TitleReference {3.1.1}{RIP and Stable Embeddings}}{21}{RIP and Stable Embeddings}{equation.3.1.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Random Matrix Constructions}{21}{subsection.3.1.2}}
\newlabel{sec:mtx-contruction}{{\M@TitleReference {3.1.2}{Random Matrix Constructions}}{21}{Random Matrix Constructions}{subsection.3.1.2}{}}
\newlabel{cond:norm-pres}{{\M@TitleReference {1}{Random Matrix Constructions}}{21}{Norm preservation}{condition.1}{}}
\newlabel{cond:sub-Gauss}{{\M@TitleReference {2}{Random Matrix Constructions}}{21}{sub-Gaussian}{condition.2}{}}
\newlabel{cond:sub-Gauss concetration}{{3.1.2.10}{21}{sub-Gaussian}{equation.3.1.10}{}}
\citation{levequeMatrices}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Wishart Matrices}{22}{subsection.3.1.3}}
\citation{Chen1998a}
\citation{tibshirani1996regression}
\citation{hastie2005elements}
\newlabel{remark: exp AtA}{{\M@TitleReference {3.1.16}{Wishart Matrices}}{23}{}{theorem.3.1.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Reconstruction Algorithms}{23}{subsection.3.1.4}}
\newlabel{program:bp}{{3.1.4.16}{23}{Reconstruction Algorithms}{equation.3.1.16}{}}
\newlabel{program:lasso}{{3.1.4.17}{23}{Reconstruction Algorithms}{equation.3.1.17}{}}
\newlabel{soln:lasso}{{3.1.4.18}{23}{Reconstruction Algorithms}{equation.3.1.18}{}}
\citation{Tibshirani1996}
\citation{Tibshirani1996}
\newlabel{program:ridge}{{3.1.4.19}{24}{Reconstruction Algorithms}{equation.3.1.19}{}}
\newlabel{soln:ridge}{{3.1.4.20}{24}{Reconstruction Algorithms}{equation.3.1.20}{}}
\newlabel{program:ell0}{{3.1.4.21}{24}{Reconstruction Algorithms}{equation.3.1.21}{}}
\newlabel{soln:l0}{{3.1.4.22}{24}{Reconstruction Algorithms}{equation.3.1.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Solutions to the Compressive Sensing optimisation problem intersect the \(l_1\) norm the points where all components (but one) of the vector are zero (i.e. it is sparsity promoting) \cite  {Tibshirani1996}}}{24}{figure.3.2}}
\newlabel{fig:l1l2}{{\M@TitleReference {3.2}{Solutions to the Compressive Sensing optimisation problem intersect the \(l_1\) norm the points where all components (but one) of the vector are zero (i.e. it is sparsity promoting) \cite  {Tibshirani1996}}}{24}{Solutions to the Compressive Sensing optimisation problem intersect the \(l_1\) norm the points where all components (but one) of the vector are zero (i.e. it is sparsity promoting) \cite {Tibshirani1996}}{figure.3.2}{}}
\citation{candes2007dantzig}
\citation{candes2007dantzig}
\citation{bickel2009simultaneous}
\newlabel{program:enat}{{3.1.4.23}{25}{Reconstruction Algorithms}{equation.3.1.23}{}}
\newlabel{program:enat}{{3.1.4.24}{25}{Reconstruction Algorithms}{equation.3.1.24}{}}
\citation{figueiredo2003algorithm}
\citation{tropp2007signal}
\citation{dai2009subspace}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The Iterative Soft Thresholding Algorithm}}{26}{figure.3.3}}
\newlabel{alg:IST}{{\M@TitleReference {3.3}{The Iterative Soft Thresholding Algorithm}}{26}{The Iterative Soft Thresholding Algorithm}{figure.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The OMP recovery algorithm}}{26}{figure.3.4}}
\newlabel{alg:omp}{{\M@TitleReference {3.4}{The OMP recovery algorithm}}{26}{The OMP recovery algorithm}{figure.3.4}{}}
\citation{goldberger1961stepwise}
\citation{pati1993orthogonal}
\citation{mallat1993matching}
\citation{tropp2007signal}
\citation{wen2013improved}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The AMP recovery algorithm}}{27}{figure.3.5}}
\newlabel{alg:amp}{{\M@TitleReference {3.5}{The AMP recovery algorithm}}{27}{The AMP recovery algorithm}{figure.3.5}{}}
\citation{Tibshirani1996}
\citation{Tibshirani1996}
\citation{Tibshirani1996}
\citation{Baron2010}
\newlabel{CSequation}{{\M@TitleReference {3.1.4}{Reconstruction Algorithms}}{28}{Reconstruction Algorithms}{equation.3.1.27}{}}
\citation{Ji2008}
\citation{Ji2008}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces The Laplace (\(l_1\)-norm, bold line) and Normal (\(l_2\)-norm, dotted line) densities. Note that the Laplace density is sparsity promoting as it penalises solutions away from zero more than the Gaussian density. \cite  {Tibshirani1996}}}{29}{figure.3.6}}
\newlabel{laplacenormal}{{\M@TitleReference {3.6}{The Laplace (\(l_1\)-norm, bold line) and Normal (\(l_2\)-norm, dotted line) densities. Note that the Laplace density is sparsity promoting as it penalises solutions away from zero more than the Gaussian density. \cite  {Tibshirani1996}}}{29}{The Laplace (\(l_1\)-norm, bold line) and Normal (\(l_2\)-norm, dotted line) densities. Note that the Laplace density is sparsity promoting as it penalises solutions away from zero more than the Gaussian density. \cite {Tibshirani1996}}{figure.3.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces The hierarchical model for the Bayesian CS formulation \cite  {Ji2008}}}{29}{figure.3.7}}
\newlabel{bayesiancs}{{\M@TitleReference {3.7}{The hierarchical model for the Bayesian CS formulation \cite  {Ji2008}}}{29}{The hierarchical model for the Bayesian CS formulation \cite {Ji2008}}{figure.3.7}{}}
\citation{Yedidia2011}
\citation{metzler2014denoising}
\citation{Zhang2011b}
\citation{Zhang2011b}
\citation{mishali2010theory}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Multiple Measurement Vector}{30}{section.3.2}}
\newlabel{sec:mmv}{{\M@TitleReference {3.2}{Multiple Measurement Vector}}{30}{Multiple Measurement Vector}{section.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Compressive Sensing Architechtures}{30}{section.3.3}}
\newlabel{sec:sensingmodel}{{\M@TitleReference {3.3}{Compressive Sensing Architechtures}}{30}{Compressive Sensing Architechtures}{section.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Modulated Wideband Converter}{30}{subsection.3.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Mse vs SNR for the sensing model, with AWGN only, showing the performance of distributed and centralised solvers}}{31}{figure.3.8}}
\newlabel{msevssnr0}{{\M@TitleReference {3.8}{Mse vs SNR for the sensing model, with AWGN only, showing the performance of distributed and centralised solvers}}{31}{Mse vs SNR for the sensing model, with AWGN only, showing the performance of distributed and centralised solvers}{figure.3.8}{}}
\newlabel{system}{{3.3.1.36}{31}{Modulated Wideband Converter}{equation.3.3.36}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Random Demodulator}{32}{subsection.3.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Distributed Compressive Sensing}{33}{section.3.4}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Compressive Sampling for Spectrum Sensing}{33}{section.3.5}}
\citation{parikh2014proximal}
\citation{rockafellar1976monotone}
\citation{douglas1956numerical}
\citation{eckstein1992douglas}
\citation{Bristow2014}
\citation{heredia2015consensus}
\citation{sawatzky2014proximal}
\citation{o2013splitting}
\citation{Shi2013}
\citation{nishihara2015general}
\citation{ghadimi2015optimal}
\citation{goldstein2014fast}
\citation{chen2016direct}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {4}ADMM}{35}{chapter.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}ADMM}{35}{section.4.1}}
\citation{Boyd2010a}
\citation{nesterov2005smooth}
\newlabel{LASSO}{{4.1.0.2}{36}{ADMM}{equation.4.1.2}{}}
\newlabel{LASSO-L0}{{4.1.0.3}{36}{ADMM}{equation.4.1.3}{}}
\newlabel{admm}{{4.1.0.4}{36}{ADMM}{equation.4.1.4}{}}
\newlabel{admm_form}{{4.1.0.5}{36}{ADMM}{equation.4.1.5}{}}
\newlabel{admm_algo}{{4.1.0.7}{36}{ADMM}{equation.4.1.7}{}}
\newlabel{eq:lasso-lagrangian}{{4.1.0.10}{37}{ADMM}{equation.4.1.10}{}}
\newlabel{admm_algo_lasso}{{4.1.0.13}{37}{ADMM}{equation.4.1.13}{}}
\newlabel{dellx}{{4.1.0.14}{37}{ADMM}{equation.4.1.14}{}}
\newlabel{optx}{{4.1.0.16}{37}{ADMM}{equation.4.1.16}{}}
\newlabel{dellz-positive}{{4.1.0.17}{38}{ADMM}{equation.4.1.17}{}}
\newlabel{dellz-negative}{{4.1.0.18}{38}{ADMM}{equation.4.1.18}{}}
\newlabel{zbounds}{{4.1.0.19}{38}{ADMM}{equation.4.1.19}{}}
\newlabel{optz}{{4.1.0.20}{38}{ADMM}{equation.4.1.20}{}}
\citation{moreau1965proximite}
\newlabel{hatx}{{4.1.0.23}{39}{ADMM}{equation.4.1.23}{}}
\citation{moreau1965proximite}
\citation{moreau1965proximite}
\citation{moreau1965proximite}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}The Proximity Operator}{40}{subsection.4.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{Properties}{40}{section*.6}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{41}{section*.7}}
\@writefile{toc}{\contentsline {subsubsection}{Examples}{42}{section*.8}}
\newlabel{consensus}{{4.1.12}{43}{Consensus}{theorem.4.1.12}{}}
\newlabel{admm_consensus}{{4.1.12}{43}{Consensus}{theorem.4.1.12}{}}
\newlabel{consensus_iterations}{{4.1.1.56}{43}{Consensus}{equation.4.1.56}{}}
\newlabel{simple_consensus_iterations}{{4.1.1.58}{43}{Consensus}{equation.4.1.58}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Statistical Interpretation}{44}{subsection.4.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Acceleration}{44}{subsection.4.1.3}}
\citation{Zhang2011b}
\citation{mota2013d}
\citation{mota2013d}
\citation{mota2013d}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {5}Optimisation on Graphs}{45}{chapter.5}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Constrained Optimisation on Graphs}{45}{section.5.1}}
\newlabel{sec:opt-on-graphs}{{\M@TitleReference {5.1}{Constrained Optimisation on Graphs}}{45}{Constrained Optimisation on Graphs}{section.5.1}{}}
\newlabel{constrainedbp}{{5.1.0.2}{46}{Constrained Optimisation on Graphs}{equation.5.1.2}{}}
\newlabel{constrainedbp}{{5.1.0.3}{46}{Constrained Optimisation on Graphs}{equation.5.1.3}{}}
\newlabel{barxc}{{5.1.0.4}{46}{}{equation.5.1.4}{}}
\citation{Boyd2010a}
\citation{Boyd2010a}
\newlabel{compact-constraints}{{5.1.0.5}{47}{Constrained Optimisation on Graphs}{equation.5.1.5}{}}
\newlabel{constrainedbp1}{{5.1.0.6}{47}{Constrained Optimisation on Graphs}{equation.5.1.6}{}}
\newlabel{aug-lagrange}{{5.1.0.7}{47}{Constrained Optimisation on Graphs}{equation.5.1.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces An example of a network}}{48}{figure.5.1}}
\newlabel{efig:ex-network}{{\M@TitleReference {5.1}{An example of a network}}{48}{An example of a network}{figure.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces The incidence matrix associated with Figure \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {efig:ex-network}\unskip \@@italiccorr )}}}}{48}{figure.5.2}}
\newlabel{fig:incidence-matrix}{{\M@TitleReference {5.2}{The incidence matrix associated with Figure \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {efig:ex-network}\unskip \@@italiccorr )}}}}{48}{The incidence matrix associated with Figure \eqref {efig:ex-network}}{figure.5.2}{}}
\newlabel{generic-iterations}{{5.1.0.11}{49}{Constrained Optimisation on Graphs}{equation.5.1.11}{}}
\newlabel{dadmm_algo_lasso}{{5.1.0.16}{49}{}{equation.5.1.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}DADMM-Lasso}{50}{subsection.5.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}DADMM-Dantzig-Selector}{50}{subsection.5.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}DADMM-MMV}{50}{subsection.5.1.3}}
\newlabel{generic-iterations}{{5.1.3.19}{50}{DADMM-MMV}{equation.5.1.19}{}}
\newlabel{dadmm_algo_mmv}{{5.1.3.23}{51}{DADMM-MMV}{equation.5.1.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces An synthetic (time-domain) signal for the MMV model.}}{51}{figure.5.3}}
\newlabel{fig:mmv_orig}{{\M@TitleReference {5.3}{An synthetic (time-domain) signal for the MMV model.}}{51}{An synthetic (time-domain) signal for the MMV model}{figure.5.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Results}{51}{section.5.2}}
\newlabel{sec:results}{{\M@TitleReference {5.2}{Results}}{51}{Results}{section.5.2}{}}
\citation{bazerque2008}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Example reconstruction in the MMV model.}}{52}{figure.5.4}}
\newlabel{fig:mmv_recon}{{\M@TitleReference {5.4}{Example reconstruction in the MMV model.}}{52}{Example reconstruction in the MMV model}{figure.5.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Conclusions}{52}{section.5.3}}
\citation{goldstein2014fast}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Mse vs SNR for the sensing model, with AWGN only, showing the performance of distributed and centralised solvers}}{53}{figure.5.5}}
\newlabel{msevssnr0}{{\M@TitleReference {5.5}{Mse vs SNR for the sensing model, with AWGN only, showing the performance of distributed and centralised solvers}}{53}{Mse vs SNR for the sensing model, with AWGN only, showing the performance of distributed and centralised solvers}{figure.5.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Mse vs SNR for the sensing model, showing the performance of distributed and centralised solvers}}{53}{figure.5.6}}
\newlabel{msevssnr1}{{\M@TitleReference {5.6}{Mse vs SNR for the sensing model, showing the performance of distributed and centralised solvers}}{53}{Mse vs SNR for the sensing model, showing the performance of distributed and centralised solvers}{figure.5.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \(\lambda \)}}{54}{figure.5.7}}
\newlabel{fig:differentLambda}{{\M@TitleReference {5.7}{The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \(\lambda \)}}{54}{The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \(\lambda \)}{figure.5.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{54}{figure.5.8}}
\newlabel{fig:erroriterations}{{\M@TitleReference {5.8}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{54}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{55}{figure.5.9}}
\newlabel{fig:spline_recon}{{\M@TitleReference {5.9}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{55}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{55}{figure.5.10}}
\newlabel{fig:steps_wavelets}{{\M@TitleReference {5.10}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{55}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{56}{figure.5.11}}
\newlabel{fig:wavelet_recon}{{\M@TitleReference {5.11}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{56}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{56}{figure.5.12}}
\newlabel{fig:wavelet_recon_no_pwer_2}{{\M@TitleReference {5.12}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{56}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{57}{figure.5.13}}
\newlabel{fig:erroriterations}{{\M@TitleReference {5.13}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{57}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{57}{figure.5.14}}
\newlabel{fig:steps_difference}{{\M@TitleReference {5.14}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{57}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{58}{figure.5.15}}
\newlabel{fig:steps_splines}{{\M@TitleReference {5.15}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{58}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{58}{figure.5.16}}
\newlabel{fig:steps_splines}{{\M@TitleReference {5.16}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}}{58}{The progress of a distributed (blue) and a centralised (green) solver as a function of the number of iterations. The value of \(\lambda = 0.1\)}{figure.5.16}{}}
\citation{akan2009cognitive}
\citation{Candes2006}
\citation{mishali2010theory}
\citation{polo2009compressive}
\citation{tropp2010beyond}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {6}Sensing with Heavyside Basis}{59}{chapter.6}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{59}{section.6.1}}
\citation{Zhang2011b}
\citation{tian2006wavelet}
\citation{tian2006wavelet}
\citation{ling2015dlm}
\citation{mokhtari2015dqm}
\citation{mota2013d}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Signal Model}{60}{section.6.2}}
\newlabel{basis}{{6.2.0.1}{61}{Signal Model}{equation.6.2.1}{}}
\newlabel{basis-expansion}{{6.2.0.4}{61}{Signal Model}{equation.6.2.4}{}}
\newlabel{def:a}{{\M@TitleReference {6.2.1}{Signal Model}}{61}{}{equation.6.2.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Sensing Model}{62}{section.6.3}}
\newlabel{sec:sensingmodel}{{\M@TitleReference {6.3}{Sensing Model}}{62}{Sensing Model}{section.6.3}{}}
\newlabel{dist_system}{{6.3.0.9}{62}{Sensing Model}{equation.6.3.9}{}}
\newlabel{system}{{6.3.0.10}{62}{Sensing Model}{equation.6.3.10}{}}
\newlabel{opt}{{6.3.0.11}{62}{Sensing Model}{equation.6.3.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Constrained Optimisation on Graphs}{62}{section.6.4}}
\newlabel{sec:opt-on-graphs}{{\M@TitleReference {6.4}{Constrained Optimisation on Graphs}}{62}{Constrained Optimisation on Graphs}{section.6.4}{}}
\newlabel{barxc}{{6.4.0.13}{63}{}{equation.6.4.13}{}}
\newlabel{constrainedbp}{{6.4.0.14}{63}{Constrained Optimisation on Graphs}{equation.6.4.14}{}}
\newlabel{compact-constraints}{{6.4.0.15}{63}{Constrained Optimisation on Graphs}{equation.6.4.15}{}}
\citation{Boyd2010a}
\citation{Boyd2010a}
\newlabel{constrainedbp1}{{6.4.0.16}{64}{Constrained Optimisation on Graphs}{equation.6.4.16}{}}
\newlabel{aug-lagrange}{{6.4.0.17}{64}{Constrained Optimisation on Graphs}{equation.6.4.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces The algorithm at Node \(j\)}}{66}{figure.6.1}}
\newlabel{DADMM}{{\M@TitleReference {6.1}{The algorithm at Node \(j\)}}{66}{The algorithm at Node \(j\)}{figure.6.1}{}}
\newlabel{generic-iterations}{{6.4.0.21}{66}{Constrained Optimisation on Graphs}{equation.6.4.21}{}}
\newlabel{dadmm_algo_lasso}{{6.4.0.26}{66}{Constrained Optimisation on Graphs}{equation.6.4.26}{}}
\citation{Chen1998}
\citation{bazerque2008}
\citation{bazerque2008}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Results}{67}{section.6.5}}
\newlabel{sec:results}{{\M@TitleReference {6.5}{Results}}{67}{Results}{section.6.5}{}}
\citation{shi2014linear}
\citation{nishihara2015general}
\citation{su2014differential}
\citation{mokhtari2015dqm}
\citation{ling2015dlm}
\citation{goldstein2014fast}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Conclusions}{68}{section.6.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Left to right: (a) The original signal. (b) The gradient \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {def:a}\unskip \@@italiccorr )}} of the original signal. (c) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 5\). (d) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 20\) }}{69}{figure.6.2}}
\newlabel{different_sigs}{{\M@TitleReference {6.2}{Left to right: (a) The original signal. (b) The gradient \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {def:a}\unskip \@@italiccorr )}} of the original signal. (c) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 5\). (d) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 20\) }}{69}{Left to right: (a) The original signal. (b) The gradient \eqref {def:a} of the original signal. (c) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 5\). (d) Recovery using DADMM, 1000 iterations, \(\sigma ^2_n = 20\) }{figure.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces MSE vs SNR for the sensing model showing the performance of distributed and centralised solvers. The performance of DADMM is consistently within \(10^{-2}\) of ADMM, and within the error bars of ADMM at low SNRs. The variance of estimates produced by DADMM is larger than ADMM, due to nodes performing computations on a subset of data. Both estimates are consistently within \(10^{-1}\) of the optimal solution, which is sufficient to classify occupied bands.}}{70}{figure.6.3}}
\newlabel{msevssnr0}{{\M@TitleReference {6.3}{MSE vs SNR for the sensing model showing the performance of distributed and centralised solvers. The performance of DADMM is consistently within \(10^{-2}\) of ADMM, and within the error bars of ADMM at low SNRs. The variance of estimates produced by DADMM is larger than ADMM, due to nodes performing computations on a subset of data. Both estimates are consistently within \(10^{-1}\) of the optimal solution, which is sufficient to classify occupied bands.}}{70}{MSE vs SNR for the sensing model showing the performance of distributed and centralised solvers. The performance of DADMM is consistently within \(10^{-2}\) of ADMM, and within the error bars of ADMM at low SNRs. The variance of estimates produced by DADMM is larger than ADMM, due to nodes performing computations on a subset of data. Both estimates are consistently within \(10^{-1}\) of the optimal solution, which is sufficient to classify occupied bands}{figure.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \( \lambda \). For a fixed \( \lambda \) there is a single unique optimal solution, with higher \( \lambda \) favouring sparser solutions. The convergence of DADMM is slowed by smaller \( \lambda \). This is intuitive: solutions with fewer non-zero components should be identified in fewer iterations.}}{70}{figure.6.4}}
\newlabel{fig:differentLambda}{{\M@TitleReference {6.4}{The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \( \lambda \). For a fixed \( \lambda \) there is a single unique optimal solution, with higher \( \lambda \) favouring sparser solutions. The convergence of DADMM is slowed by smaller \( \lambda \). This is intuitive: solutions with fewer non-zero components should be identified in fewer iterations.}}{70}{The progress of the distributed solver as a function of the number of iterations, with different values of the regression parameter \( \lambda \). For a fixed \( \lambda \) there is a single unique optimal solution, with higher \( \lambda \) favouring sparser solutions. The convergence of DADMM is slowed by smaller \( \lambda \). This is intuitive: solutions with fewer non-zero components should be identified in fewer iterations}{figure.6.4}{}}
\citation{Candes2006}
\citation{Donoho2006}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {7}Compressive Inference}{71}{chapter.7}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Introduction}{71}{section.7.1}}
\citation{Candes2006}
\citation{donoho2004neighborly}
\newlabel{minsamples}{{\M@TitleReference {7.1}{Introduction}}{72}{Introduction}{equation.7.1.7}{}}
\citation{davenport2010signal}
\citation{davenport2007smashed}
\citation{schnelle2012compressive}
\citation{davenport2010wideband}
\citation{eftekhari2013matched}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Compressive Estimation}{73}{section.7.2}}
\newlabel{sec:estimation}{{\M@TitleReference {7.2}{Compressive Estimation}}{73}{Compressive Estimation}{section.7.2}{}}
\newlabel{log-like}{{7.2.0.12}{74}{Compressive Estimation}{equation.7.2.12}{}}
\newlabel{approx-log-like}{{7.2.0.16}{74}{Compressive Estimation}{equation.7.2.16}{}}
\newlabel{eq: compressive-estimator}{{7.2.0.17}{74}{Compressive Estimation}{equation.7.2.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Example: Single Spike}{75}{subsection.7.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces }}{76}{figure.7.1}}
\newlabel{fig:new_basis_25}{{\M@TitleReference {7.1}{}}{76}{}{figure.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Estimating a single rectangle}{76}{subsection.7.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces }}{76}{figure.7.2}}
\newlabel{fig:rectangle}{{\M@TitleReference {7.2}{}}{76}{}{figure.7.2}{}}
\newlabel{basis}{{7.2.2.23}{76}{Estimating a single rectangle}{equation.7.2.23}{}}
\newlabel{basis-expansion}{{7.2.2.24}{77}{Estimating a single rectangle}{equation.7.2.24}{}}
\newlabel{ss-estimator}{{7.2.2.32}{77}{Estimating a single rectangle}{equation.7.2.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.3}Estimating Frequency spectra}{77}{subsection.7.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces }}{78}{figure.7.3}}
\newlabel{fig:hhat}{{\M@TitleReference {7.3}{}}{78}{}{figure.7.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces ROC for synthetic data, midly noisy}}{78}{figure.7.4}}
\newlabel{fig:hvb}{{\M@TitleReference {7.4}{ROC for synthetic data, midly noisy}}{78}{ROC for synthetic data, midly noisy}{figure.7.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces ROC for synthetic data, very noisy}}{79}{figure.7.5}}
\newlabel{fig:hvb}{{\M@TitleReference {7.5}{ROC for synthetic data, very noisy}}{79}{ROC for synthetic data, very noisy}{figure.7.5}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {8}Results on OFCOM Data}{81}{chapter.8}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Introduction}{81}{section.8.1}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Data Set}{81}{section.8.2}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Results: Distributed Estimation with Heaviside Basis}{81}{section.8.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Example of classification with OFCOM data, 35 changepoints}}{81}{figure.8.1}}
\newlabel{fig:hvb}{{\M@TitleReference {8.1}{Example of classification with OFCOM data, 35 changepoints}}{81}{Example of classification with OFCOM data, 35 changepoints}{figure.8.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Example of classification with OFCOM data, 55 changepoints}}{82}{figure.8.2}}
\newlabel{fig:hvb}{{\M@TitleReference {8.2}{Example of classification with OFCOM data, 55 changepoints}}{82}{Example of classification with OFCOM data, 55 changepoints}{figure.8.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces Example of classification with OFCOM data, 35 changepoints}}{82}{figure.8.3}}
\newlabel{fig:hvb}{{\M@TitleReference {8.3}{Example of classification with OFCOM data, 35 changepoints}}{82}{Example of classification with OFCOM data, 35 changepoints}{figure.8.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.4}Compressive Estimation}{82}{section.8.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces Example of classification with OFCOM data, 55 changepoints}}{83}{figure.8.4}}
\newlabel{fig:hvb}{{\M@TitleReference {8.4}{Example of classification with OFCOM data, 55 changepoints}}{83}{Example of classification with OFCOM data, 55 changepoints}{figure.8.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces Example of classification with OFCOM data, 35 changepoints}}{83}{figure.8.5}}
\newlabel{fig:hvb}{{\M@TitleReference {8.5}{Example of classification with OFCOM data, 35 changepoints}}{83}{Example of classification with OFCOM data, 35 changepoints}{figure.8.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces Example of classification with OFCOM data, 55 changepoints}}{84}{figure.8.6}}
\newlabel{fig:hvb}{{\M@TitleReference {8.6}{Example of classification with OFCOM data, 55 changepoints}}{84}{Example of classification with OFCOM data, 55 changepoints}{figure.8.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.7}{\ignorespaces Example of classification with OFCOM data, 35 changepoints}}{84}{figure.8.7}}
\newlabel{fig:hvb}{{\M@TitleReference {8.7}{Example of classification with OFCOM data, 35 changepoints}}{84}{Example of classification with OFCOM data, 35 changepoints}{figure.8.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.8}{\ignorespaces Example of classification with OFCOM data, 55 changepoints}}{85}{figure.8.8}}
\newlabel{fig:hvb}{{\M@TitleReference {8.8}{Example of classification with OFCOM data, 55 changepoints}}{85}{Example of classification with OFCOM data, 55 changepoints}{figure.8.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.9}{\ignorespaces Example of classification with OFCOM data, 35 changepoints}}{85}{figure.8.9}}
\newlabel{fig:hvb}{{\M@TitleReference {8.9}{Example of classification with OFCOM data, 35 changepoints}}{85}{Example of classification with OFCOM data, 35 changepoints}{figure.8.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.10}{\ignorespaces Example of classification with OFCOM data, 55 changepoints}}{86}{figure.8.10}}
\newlabel{fig:hvb}{{\M@TitleReference {8.10}{Example of classification with OFCOM data, 55 changepoints}}{86}{Example of classification with OFCOM data, 55 changepoints}{figure.8.10}{}}
\citation{Dorfman1943}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {9}Group Testing}{87}{chapter.9}}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Introduction and notation}{87}{section.9.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.1}Group Testing}{87}{subsection.9.1.1}}
\citation{atia2}
\citation{atia2}
\citation{du}
\citation{Hwang1972}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces The Group Testing model: multiplication with a short, fat matrix \cite  {atia2}}}{89}{figure.9.1}}
\newlabel{bayesiancs}{{\M@TitleReference {9.1}{The Group Testing model: multiplication with a short, fat matrix \cite  {atia2}}}{89}{The Group Testing model: multiplication with a short, fat matrix \cite {atia2}}{figure.9.1}{}}
\citation{Aldridge2013}
\citation{Chan2011}
\newlabel{hwangbound}{{\M@TitleReference {9.1.1}{Bounds}}{90}{Bounds}{equation.9.1.9}{}}
\citation{Baldassini2013}
\citation{Emma}
\citation{Sejdinovic2010}
\citation{Wadayama}
\citation{Baldassini2013}
\newlabel{compbound}{{\M@TitleReference {9.1.1}{Bounds}}{91}{Bounds}{equation.9.1.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{Comparison to Compressive Sensing}{91}{section*.12}}
\citation{dorfman}
\citation{dorfman}
\citation{du,malyutov}
\citation{malyutov}
\citation{atia,johnsonc8,johnson33}
\citation{Wadayama}
\citation{li5}
\citation{atia2}
\citation{dorfman}
\citation{shental}
\citation{atia,johnsonc10,johnson33,tan}
\citation{atia,tan}
\citation{johnsonc10}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.2}The Probabilistic group testing problem}{92}{subsection.9.1.2}}
\citation{johnsonc10}
\citation{li5}
\citation{johnsonc10}
\citation{johnsonc10}
\citation{johnsonc10}
\citation{hwang}
\citation{du}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.3}Group testing capacity}{93}{subsection.9.1.3}}
\newlabel{def:capacity}{{\M@TitleReference {9.1.1}{Group testing capacity}}{93}{}{theorem.9.1.1}{}}
\newlabel{eq:lower}{{9.1.3.11}{93}{}{equation.9.1.11}{}}
\newlabel{eq:upper}{{9.1.3.12}{93}{}{equation.9.1.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.4}Main results}{93}{subsection.9.1.4}}
\newlabel{thm:mainold}{{\M@TitleReference {9.1.4}{Main results}}{93}{\cite {johnsonc10}}{theorem.9.1.4}{}}
\newlabel{eq:bja}{{9.1.4.13}{93}{Main results}{equation.9.1.13}{}}
\citation{johnsonc10}
\citation{}
\citation{li5}
\citation{li5}
\citation{hwang}
\citation{li5}
\citation{johnsonc10}
\newlabel{cor:main}{{\M@TitleReference {9.1.5}{Main results}}{94}{}{theorem.9.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Algorithms and existing results}{94}{section.9.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.1}Upper bounds on success probability}{94}{subsection.9.2.1}}
\newlabel{sec:ub}{{\M@TitleReference {9.2.1}{Upper bounds on success probability}}{94}{Upper bounds on success probability}{subsection.9.2.1}{}}
\newlabel{thm:upper}{{\M@TitleReference {9.2.1}{Upper bounds on success probability}}{94}{}{theorem.9.2.1}{}}
\citation{li5}
\citation{li5}
\citation{li5}
\citation{li5}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.2}Binary search algorithms}{95}{subsection.9.2.2}}
\newlabel{thm:lower}{{\M@TitleReference {9.2.3}{Binary search algorithms}}{95}{}{theorem.9.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.3}Summary of our contribution}{95}{subsection.9.2.3}}
\newlabel{sec:algo}{{\M@TitleReference {9.2.3}{Summary of our contribution}}{95}{Summary of our contribution}{subsection.9.2.3}{}}
\citation{li5}
\citation{aksoylar,tan}
\citation{Candes2006,donoho2}
\citation{Candes2006,donoho2}
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces Algorithm for the non-iid group testing problem}}{96}{figure.9.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.4}Wider context: sparse inference problems}{96}{subsection.9.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Analysis and new bounds}{97}{section.9.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.1}Searching a set of bounded ratio}{97}{subsection.9.3.1}}
\newlabel{sec:boundedratio}{{\M@TitleReference {9.3.1}{Searching a set of bounded ratio}}{97}{Searching a set of bounded ratio}{subsection.9.3.1}{}}
\newlabel{cond:ratio}{{\M@TitleReference {3}{Searching a set of bounded ratio}}{97}{Bounded Ratio Condition}{condition.3}{}}
\newlabel{eq:ratio}{{9.3.1.14}{97}{Bounded Ratio Condition}{equation.9.3.14}{}}
\newlabel{lem:sfstep}{{\M@TitleReference {9.3.1}{Searching a set of bounded ratio}}{97}{}{theorem.9.3.1}{}}
\newlabel{eq:depth}{{9.3.1.15}{97}{}{equation.9.3.15}{}}
\newlabel{eq:setbd}{{9.3.1.16}{97}{Searching a set of bounded ratio}{equation.9.3.16}{}}
\newlabel{eq:lengthbd}{{\M@TitleReference {9.3.1.17}{Searching a set of bounded ratio}}{97}{Searching a set of bounded ratio}{equation.9.3.17}{}}
\citation{li5}
\newlabel{rem:algo}{{\M@TitleReference {9.3.2}{Searching a set of bounded ratio}}{98}{}{theorem.9.3.2}{}}
\newlabel{lem:expset}{{\M@TitleReference {9.3.3}{Searching a set of bounded ratio}}{98}{}{theorem.9.3.3}{}}
\newlabel{eq:tbds}{{9.3.1.18}{98}{}{equation.9.3.18}{}}
\newlabel{eq:testsperdef}{{\M@TitleReference {9.3.1.19}{Searching a set of bounded ratio}}{98}{Searching a set of bounded ratio}{equation.9.3.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.2}Discarding low probability items}{98}{subsection.9.3.2}}
\newlabel{sec:discard}{{\M@TitleReference {9.3.2}{Discarding low probability items}}{98}{Discarding low probability items}{subsection.9.3.2}{}}
\citation{li5}
\newlabel{lem:thresh}{{\M@TitleReference {9.3.4}{Discarding low probability items}}{99}{}{theorem.9.3.4}{}}
\newlabel{eq:thetadef}{{9.3.2.20}{99}{}{equation.9.3.20}{}}
\newlabel{eq:setpstar}{{9.3.2.21}{99}{}{equation.9.3.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.3}Searching the entire set}{99}{subsection.9.3.3}}
\newlabel{eq:bincount}{{9.3.3.22}{99}{Searching the entire set}{equation.9.3.22}{}}
\newlabel{def:full}{{\M@TitleReference {9.3.5}{Searching the entire set}}{99}{}{theorem.9.3.5}{}}
\newlabel{prop:splitting}{{\M@TitleReference {9.3.6}{Searching the entire set}}{99}{}{theorem.9.3.6}{}}
\newlabel{it:count}{{\M@TitleReference {1}{Searching the entire set}}{100}{Searching the entire set}{Item.18}{}}
\newlabel{eq:counting}{{9.3.3.23}{100}{Searching the entire set}{equation.9.3.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.4}Bounding the expected number of tests}{100}{subsection.9.3.4}}
\newlabel{sec:expectation}{{\M@TitleReference {9.3.4}{Bounding the expected number of tests}}{100}{Bounding the expected number of tests}{subsection.9.3.4}{}}
\newlabel{prop:overall}{{\M@TitleReference {9.3.7}{Bounding the expected number of tests}}{100}{}{theorem.9.3.7}{}}
\newlabel{eq:tbd}{{9.3.4.24}{100}{}{equation.9.3.24}{}}
\citation{petrov}
\newlabel{eq:total}{{\M@TitleReference {9.3.4.25}{Bounding the expected number of tests}}{101}{Bounding the expected number of tests}{equation.9.3.25}{}}
\newlabel{eq:toopt}{{\M@TitleReference {9.3.4.25}{Bounding the expected number of tests}}{101}{Bounding the expected number of tests}{equation.9.3.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.5}Controlling the error probabilities}{101}{subsection.9.3.5}}
\newlabel{sec:main}{{\M@TitleReference {9.3.5}{Controlling the error probabilities}}{101}{Controlling the error probabilities}{subsection.9.3.5}{}}
\newlabel{thm:bernstein}{{\M@TitleReference {9.3.8}{Controlling the error probabilities}}{101}{Bernstein}{theorem.9.3.8}{}}
\newlabel{eq:bernstein}{{9.3.5.26}{101}{Bernstein}{equation.9.3.26}{}}
\newlabel{thm:main}{{\M@TitleReference {9.3.9}{Controlling the error probabilities}}{101}{}{theorem.9.3.9}{}}
\newlabel{eq:tnec}{{9.3.5.27}{101}{}{equation.9.3.27}{}}
\newlabel{it:part1}{{\M@TitleReference {1}{Controlling the error probabilities}}{101}{}{Item.21}{}}
\newlabel{eq:errorprob}{{9.3.5.28}{101}{}{equation.9.3.28}{}}
\newlabel{it:part2}{{\M@TitleReference {2}{Controlling the error probabilities}}{102}{}{Item.22}{}}
\newlabel{eq:vbd}{{9.3.5.29}{102}{Controlling the error probabilities}{equation.9.3.29}{}}
\newlabel{eq:ratio2}{{\M@TitleReference {9.3.5.30}{Controlling the error probabilities}}{102}{Controlling the error probabilities}{equation.9.3.30}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.4}Results}{103}{section.9.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces Theoretical lower and upper bounds and empirical Test frequencies as functions of \(\theta \)}}{103}{figure.9.3}}
\newlabel{ubvslb}{{\M@TitleReference {9.3}{Theoretical lower and upper bounds and empirical Test frequencies as functions of \(\theta \)}}{103}{Theoretical lower and upper bounds and empirical Test frequencies as functions of \(\theta \)}{figure.9.3}{}}
\citation{li5}
\@writefile{lof}{\contentsline {figure}{\numberline {9.4}{\ignorespaces Cumulative distribution curves of the modified Hwang algorithm with fixed \(\theta = 0.0001\) and \(\alpha \) varying }}{104}{figure.9.4}}
\newlabel{testsvsalpha}{{\M@TitleReference {9.4}{Cumulative distribution curves of the modified Hwang algorithm with fixed \(\theta = 0.0001\) and \(\alpha \) varying }}{104}{Cumulative distribution curves of the modified Hwang algorithm with fixed \(\theta = 0.0001\) and \(\alpha \) varying }{figure.9.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.5}{\ignorespaces Cumulative distribution curves for fixed \(\alpha = 1\) and varying \(\theta \)}}{104}{figure.9.5}}
\newlabel{testsvstheta}{{\M@TitleReference {9.5}{Cumulative distribution curves for fixed \(\alpha = 1\) and varying \(\theta \)}}{104}{Cumulative distribution curves for fixed \(\alpha = 1\) and varying \(\theta \)}{figure.9.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.5}Discussion}{104}{section.9.5}}
\bibdata{thesis}
\bibcite{akan2009cognitive}{{1}{}{{}}{{}}}
\bibcite{aksoylar}{{2}{}{{}}{{}}}
\bibcite{johnson33}{{3}{}{{}}{{}}}
\bibcite{Aldridge2013}{{4}{}{{}}{{}}}
\bibcite{atia2}{{5}{}{{}}{{}}}
\bibcite{atia}{{6}{}{{}}{{}}}
\bibcite{Baldassini2013}{{7}{}{{}}{{}}}
\bibcite{johnsonc10}{{8}{}{{}}{{}}}
\bibcite{baraniuk2008simple}{{9}{}{{}}{{}}}
\bibcite{Baron2010}{{10}{}{{}}{{}}}
\bibcite{bazerque2008}{{11}{}{{}}{{}}}
\bibcite{}{{12}{}{{}}{{}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{107}{section*.14}}
\bibcite{bhargavi2010performance}{{13}{}{{}}{{}}}
\bibcite{bickel2009simultaneous}{{14}{}{{}}{{}}}
\bibcite{Boyd2010a}{{15}{}{{}}{{}}}
\bibcite{Bristow2014}{{16}{}{{}}{{}}}
\bibcite{Burbidge2007}{{17}{}{{}}{{}}}
\bibcite{vcabric2005physical}{{18}{}{{}}{{}}}
\bibcite{cabric2004implementation}{{19}{}{{}}{{}}}
\bibcite{Emma}{{20}{}{{}}{{}}}
\bibcite{candes2007dantzig}{{21}{}{{}}{{}}}
\bibcite{Candes2006}{{22}{}{{}}{{}}}
\bibcite{Chan2011}{{23}{}{{}}{{}}}
\bibcite{chen2016direct}{{24}{}{{}}{{}}}
\bibcite{Chen1998a}{{25}{}{{}}{{}}}
\bibcite{Chen1998}{{26}{}{{}}{{}}}
\bibcite{dai2009subspace}{{27}{}{{}}{{}}}
\bibcite{davenport2010signal}{{28}{}{{}}{{}}}
\bibcite{davenport2007smashed}{{29}{}{{}}{{}}}
\bibcite{davenport2010wideband}{{30}{}{{}}{{}}}
\bibcite{donoho2004neighborly}{{31}{}{{}}{{}}}
\bibcite{donoho2}{{32}{}{{}}{{}}}
\bibcite{Donoho}{{33}{}{{}}{{}}}
\bibcite{dorfman}{{34}{}{{}}{{}}}
\bibcite{Dorfman1943}{{35}{}{{}}{{}}}
\bibcite{douglas1956numerical}{{36}{}{{}}{{}}}
\bibcite{du}{{37}{}{{}}{{}}}
\bibcite{eckstein1992douglas}{{38}{}{{}}{{}}}
\bibcite{eftekhari2013matched}{{39}{}{{}}{{}}}
\bibcite{figueiredo2003algorithm}{{40}{}{{}}{{}}}
\bibcite{ghadimi2015optimal}{{41}{}{{}}{{}}}
\bibcite{Ghozzi2006}{{42}{}{{}}{{}}}
\bibcite{goldberger1961stepwise}{{43}{}{{}}{{}}}
\bibcite{goldstein2014fast}{{44}{}{{}}{{}}}
\bibcite{hamdi2010impact}{{45}{}{{}}{{}}}
\bibcite{hastie2005elements}{{46}{}{{}}{{}}}
\bibcite{heredia2015consensus}{{47}{}{{}}{{}}}
\bibcite{hwang}{{48}{}{{}}{{}}}
\bibcite{Ji2008}{{49}{}{{}}{{}}}
\bibcite{kim2007cyclostationary}{{50}{}{{}}{{}}}
\bibcite{levequeMatrices}{{51}{}{{}}{{}}}
\bibcite{li5}{{52}{}{{}}{{}}}
\bibcite{ling2015dlm}{{53}{}{{}}{{}}}
\bibcite{lunden2007spectrum}{{54}{}{{}}{{}}}
\bibcite{mallat1993matching}{{55}{}{{}}{{}}}
\bibcite{malyutov}{{56}{}{{}}{{}}}
\bibcite{metzler2014denoising}{{57}{}{{}}{{}}}
\bibcite{mishali2010theory}{{58}{}{{}}{{}}}
\bibcite{mokhtari2015dqm}{{59}{}{{}}{{}}}
\bibcite{moreau1965proximite}{{60}{}{{}}{{}}}
\bibcite{mota2013d}{{61}{}{{}}{{}}}
\bibcite{nesterov2005smooth}{{62}{}{{}}{{}}}
\bibcite{nishihara2015general}{{63}{}{{}}{{}}}
\bibcite{o2013splitting}{{64}{}{{}}{{}}}
\bibcite{olivieri2005scalable}{{65}{}{{}}{{}}}
\bibcite{oude2011lowering}{{66}{}{{}}{{}}}
\bibcite{parikh2014proximal}{{67}{}{{}}{{}}}
\bibcite{pati1993orthogonal}{{68}{}{{}}{{}}}
\bibcite{petrov}{{69}{}{{}}{{}}}
\bibcite{polo2009compressive}{{70}{}{{}}{{}}}
\bibcite{rockafellar1976monotone}{{71}{}{{}}{{}}}
\bibcite{sahai2004some}{{72}{}{{}}{{}}}
\bibcite{sawatzky2014proximal}{{73}{}{{}}{{}}}
\bibcite{schnelle2012compressive}{{74}{}{{}}{{}}}
\bibcite{Sejdinovic2010}{{75}{}{{}}{{}}}
\bibcite{johnsonc8}{{76}{}{{}}{{}}}
\bibcite{shalev2014understanding}{{77}{}{{}}{{}}}
\bibcite{shental}{{78}{}{{}}{{}}}
\bibcite{shi2014linear}{{79}{}{{}}{{}}}
\bibcite{Shi2013}{{80}{}{{}}{{}}}
\bibcite{su2014differential}{{81}{}{{}}{{}}}
\bibcite{tan}{{82}{}{{}}{{}}}
\bibcite{tandra2008snr}{{83}{}{{}}{{}}}
\bibcite{tian2006wavelet}{{84}{}{{}}{{}}}
\bibcite{Tibshirani1996}{{85}{}{{}}{{}}}
\bibcite{tibshirani1996regression}{{86}{}{{}}{{}}}
\bibcite{tropp2007signal}{{87}{}{{}}{{}}}
\bibcite{tropp2010beyond}{{88}{}{{}}{{}}}
\bibcite{Wadayama}{{89}{}{{}}{{}}}
\bibcite{wen2013improved}{{90}{}{{}}{{}}}
\bibcite{xie2009optimal}{{91}{}{{}}{{}}}
\bibcite{ye2007spectrum}{{92}{}{{}}{{}}}
\bibcite{Yedidia2011}{{93}{}{{}}{{}}}
\bibcite{yucek2009survey}{{94}{}{{}}{{}}}
\bibcite{Zhang2011b}{{95}{}{{}}{{}}}
\bibcite{zhang2011adaptive}{{96}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\memsetcounter{lastsheet}{122}
\memsetcounter{lastpage}{114}
